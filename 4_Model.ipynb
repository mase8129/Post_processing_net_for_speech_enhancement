{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load tfrecords, define model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "\n",
    " # TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "print('Tensorflow Version:', tf.__version__)\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "# Helper libraries\n",
    "import os\n",
    "import os.path\n",
    "import glob\n",
    "import librosa\n",
    "import librosa.display\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import IPython.display as pd\n",
    "import pprint\n",
    "\n",
    "# Check if the GPU is available (otherwise computing will take a looooonnnnggggg time)\n",
    "print(\"GPU\", \"available (YESS!!!!)\" if tf.config.list_physical_devices(\"GPU\") else \"not available :(\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load global settings in config-dictionary\n",
    "with open('./MA_CONFIG.json', 'r') as fp:\n",
    "  config = json.load(fp)\n",
    "\n",
    "# define some extra values\n",
    "config['batch_size'] = 64\n",
    "config['shuffle_buffer_size'] = 300\n",
    "config['n_epochs'] = 5\n",
    "config['train_dataset_path'] = '/Users/marius/Documents/Uni/TU_Berlin_Master/Masterarbeit/Dataset/train.tfrecord'\n",
    "config['test_dataset_path'] = '/Users/marius/Documents/Uni/TU_Berlin_Master/Masterarbeit/Dataset/test.tfrecord'\n",
    "\n",
    "\n",
    "# print config\n",
    "print(json.dumps(config, indent=4))\n",
    "\n",
    "# save config to disk\n",
    "with open('./MA_CONFIG.json', 'w+') as fp:\n",
    "    json.dump(config, fp, sort_keys=True, indent=4)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets from disk\n",
    "train_dataset = tf.data.experimental.load(config['train_dataset_path'],\n",
    "                                   (tf.TensorSpec(shape=(441000, 1),\n",
    "                                                   dtype=tf.float32, name=None),\n",
    "                                     tf.TensorSpec(shape=(441000, 1),\n",
    "                                                  dtype=tf.float32, name=None)),\n",
    "                                                   compression='GZIP')\n",
    "\n",
    "test_dataset = tf.data.experimental.load(config['test_dataset_path'],\n",
    "                                      (tf.TensorSpec(shape=(441000, 1),\n",
    "                                                        dtype=tf.float32, name=None),\n",
    "                                        tf.TensorSpec(shape=(441000, 1),\n",
    "                                                        dtype=tf.float32, name=None)),\n",
    "                                                        compression='GZIP')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# count elements in train-dataset and save to config\n",
    "i = 0\n",
    "for d in train_dataset:\n",
    "    i = i + 1\n",
    "print(f'Number of elements in train-dataset: {i}')\n",
    "\n",
    "# count elements in test-dataset and save to config\n",
    "i = 0\n",
    "for d in test_dataset:\n",
    "    i = i + 1\n",
    "print(f'Number of elements in test-dataset: {i}')\n",
    "\n",
    "\n",
    "# batching and shuffling\n",
    "train_dataset = train_dataset.shuffle(config['shuffle_buffer_size']).batch(config['batch_size'])\n",
    "test_dataset = test_dataset.shuffle(config['shuffle_buffer_size']).batch(config['batch_size'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if datasets are loaded correctly\n",
    "for d in train_dataset:\n",
    "    print(d[0].shape)\n",
    "    print(d[1].shape)\n",
    "    break   \n",
    "\n",
    "for d in test_dataset:\n",
    "    print(d[0].shape)\n",
    "    print(d[1].shape)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# look at data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at some example data from train dataset\n",
    "wavs = train_dataset.unbatch().as_numpy_iterator()\n",
    "noisy = []\n",
    "gt = []\n",
    "\n",
    "# Setup Subplot\n",
    "nrows, ncols = 2, 2\n",
    "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, sharex=True, figsize=(16, 9))\n",
    "\n",
    "\n",
    "# iterate over dataset\n",
    "for i, sample in enumerate(wavs):\n",
    "    \n",
    "    # get the column and row by modulo and remainder\n",
    "    j = i % ncols\n",
    "    k = int(i / ncols)\n",
    "    \n",
    "    # extract noisy and produced speech file from tensors\n",
    "    wave = sample[0]\n",
    "    ground_truth = sample[1]\n",
    "        \n",
    "    # plot files\n",
    "    librosa.display.waveshow(np.squeeze(wave), x_axis='time', sr=config['sr'], ax=ax[k][j], label='test_file')\n",
    "    librosa.display.waveshow(np.squeeze(ground_truth), alpha=0.3, x_axis='time', sr=config['sr'], ax=ax[k][j], label='ground_truth')\n",
    "    ax[k][j].legend()\n",
    "    ax[k][j].axis('on')\n",
    "    ax[k][j].set_title('10s speech')  \n",
    "\n",
    "    # save speech to arrays\n",
    "    noisy.append(np.squeeze(wave))\n",
    "    gt.append(np.squeeze(ground_truth))\n",
    "    \n",
    "    if i+1 == ncols*nrows:\n",
    "        break\n",
    "    \n",
    "# adjust whitespace in between subplots        \n",
    "plt.subplots_adjust(hspace=0.25, wspace=0.15)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# listen to the audio samples\n",
    "for i in range(len(gt)):\n",
    "    print(f'----------- {i+1}. speechsnippet ---------------')\n",
    "    print('')\n",
    "    print(f'Voicefixer file')\n",
    "    pd.display(pd.Audio(noisy[i].T, rate=config['sr']))\n",
    "    print(f'corresponding produced file')\n",
    "    pd.display(pd.Audio(gt[i].T, rate=config['sr']))\n",
    "    print('')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (441000, 1)\n",
    "output_channels = 1\n",
    "\n",
    "# build model with 12 layers\n",
    "def build_model(input_shape):\n",
    "\n",
    "    # define model\n",
    "    model = keras.Sequential(name='PostNet_Conv1D')\n",
    "    model.add(keras.Input(shape=input_shape))\n",
    "\n",
    "    # add layer \n",
    "    model.add(keras.layers.Conv1D(filters=128, kernel_size=32, padding='same'))\n",
    "    model.add(keras.layers.Activation('tanh'))\n",
    "\n",
    "    # Add the remaining Conv1D layers\n",
    "    for _ in range(11):\n",
    "        model.add(keras.layers.Conv1D(filters=128, kernel_size=32, padding='same'))\n",
    "        model.add(keras.layers.Activation('tanh'))\n",
    "\n",
    "    # Add the final Conv1D layer\n",
    "    model.add(keras.layers.Conv1D(filters=output_channels, kernel_size=1, padding='same'))\n",
    "    model.add(keras.layers.Activation('tanh'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model\n",
    "model = build_model(input_shape = input_shape)\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "model.summary()\n",
    "\n",
    "model.fit(train_dataset, epochs=config['n_epochs'])\n",
    "\n",
    "#model.evaluate(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Masterarbeit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
