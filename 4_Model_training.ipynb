{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load tfrecords, define model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.9.1\n",
      "GPU not available :(\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "\n",
    " # TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "print('Tensorflow Version:', tf.__version__)\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "#Tensorboard\n",
    "%load_ext tensorboard\n",
    "\n",
    "\n",
    "# Helper libraries\n",
    "import os\n",
    "import os.path\n",
    "import glob\n",
    "import librosa\n",
    "import librosa.display\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import IPython.display as pd\n",
    "import pprint\n",
    "import datetime        \n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "\n",
    "# Check if the GPU is available (otherwise computing will take a looooonnnnggggg time)\n",
    "print(\"GPU\", \"available (YESS!!!!)\" if tf.config.list_physical_devices(\"GPU\") else \"not available :(\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"batch_size\": 8,\n",
      "    \"filter_size\": 4,\n",
      "    \"fps_noisy\": \"/Users/marius/Documents/Uni/TU_Berlin_Master/Masterarbeit/Data/allFiles/noisySpeech\",\n",
      "    \"fps_produced\": \"/Users/marius/Documents/Uni/TU_Berlin_Master/Masterarbeit/Data/allFiles/producedSpeech\",\n",
      "    \"fps_voicefixer\": \"/Users/marius/Documents/Uni/TU_Berlin_Master/Masterarbeit/Data/allFiles/voicefixerOutput\",\n",
      "    \"hop_length\": 64,\n",
      "    \"input_shape\": [\n",
      "        441000,\n",
      "        1\n",
      "    ],\n",
      "    \"kernel_size\": 2,\n",
      "    \"n_epochs\": 2,\n",
      "    \"n_fft\": 512,\n",
      "    \"n_mels\": 16,\n",
      "    \"offset\": 6,\n",
      "    \"sample_length\": 20,\n",
      "    \"shuffle_buffer_size\": 300,\n",
      "    \"sr\": 44100,\n",
      "    \"test_dataset_path\": \"../Dataset/test.tfrecord\",\n",
      "    \"train_dataset_path\": \"../Dataset/train.tfrecord\",\n",
      "    \"win_length\": 512\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# load global settings in config-dictionary\n",
    "with open('./MA_CONFIG.json', 'r') as fp:\n",
    "  config = json.load(fp)\n",
    "\n",
    "# define some extra values\n",
    "config['batch_size'] = 8\n",
    "config['shuffle_buffer_size'] = 300\n",
    "config['n_epochs'] = 2\n",
    "config['filter_size'] = 4\n",
    "config['kernel_size'] = 2\n",
    "\n",
    "# print config\n",
    "print(json.dumps(config, indent=4))\n",
    "\n",
    "# save config to disk\n",
    "with open('./MA_CONFIG.json', 'w+') as fp:\n",
    "    json.dump(config, fp, sort_keys=True, indent=4)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load tfrecords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### func for decoding tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autotune for performance\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# decode tfrecords\n",
    "def decode_tf_records(seralized_example):\n",
    "    feature_description = {\n",
    "        \"voicefixer\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"produced\": tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(seralized_example, feature_description)\n",
    "\n",
    "    voicefixer, _ = tf.audio.decode_wav(example[\"voicefixer\"], desired_channels=-1)\n",
    "    produced, _ = tf.audio.decode_wav(example[\"produced\"], desired_channels=-1)\n",
    "    \n",
    "    return voicefixer, produced\n",
    "\n",
    "\n",
    "def slicing_audio(voicefixer, produced):\n",
    "    # generate random integer between 0 and 10*44100-3*44100\n",
    "    random_int = tf.random.uniform(shape=[], minval=0, maxval=7*44100, dtype=tf.int32)\n",
    "    # slice audio\n",
    "    voicefixer = voicefixer[random_int:random_int+3*44100]\n",
    "    produced = produced[random_int:random_int+3*44100]\n",
    "    return voicefixer, produced\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-30 12:47:28.631537: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in train_dataset: 240\n"
     ]
    }
   ],
   "source": [
    "# load train tfrecords\n",
    "tfrecords_paths = glob.glob('/Users/marius/Documents/Uni/TU_Berlin_Master/Masterarbeit/Dataset/train_tfrecords/*.tfrecords')\n",
    "train_dataset = tf.data.TFRecordDataset(tfrecords_paths[:4])\n",
    "train_dataset = train_dataset.map(decode_tf_records, num_parallel_calls=AUTOTUNE)\n",
    "train_dataset = train_dataset.map(slicing_audio, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# count elements in train_dataset\n",
    "print(f'Number of elements in train_dataset: {len([d for d in train_dataset])}')\n",
    "\n",
    "# batching and shuffling\n",
    "train_dataset = train_dataset.shuffle(config['shuffle_buffer_size']).batch(config['batch_size'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in test_dataset: 120\n"
     ]
    }
   ],
   "source": [
    "# load test tfrecords\n",
    "tfrecords_paths = glob.glob('/Users/marius/Documents/Uni/TU_Berlin_Master/Masterarbeit/Dataset/test_tfrecords/*.tfrecords')\n",
    "test_dataset = tf.data.TFRecordDataset(tfrecords_paths[:2])\n",
    "test_dataset = test_dataset.map(decode_tf_records, num_parallel_calls=AUTOTUNE)\n",
    "test_dataset = test_dataset.map(slicing_audio, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# count elements in test_dataset\n",
    "print(f'Number of elements in test_dataset: {len([d for d in test_dataset])}')\n",
    "\n",
    "# batching and shuffling\n",
    "test_dataset = test_dataset.shuffle(config['shuffle_buffer_size']).batch(config['batch_size'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 132300, 1)\n",
      "(8, 132300, 1)\n",
      "(8, 132300, 1)\n",
      "(8, 132300, 1)\n"
     ]
    }
   ],
   "source": [
    "# check if datasets are loaded correctly\n",
    "for d in train_dataset:\n",
    "    print(d[0].shape)\n",
    "    print(d[1].shape)\n",
    "    break   \n",
    "\n",
    "for d in test_dataset:\n",
    "    print(d[0].shape)\n",
    "    print(d[1].shape)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# look at data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # look at some example data from train dataset\n",
    "# wavs = train_dataset.unbatch().as_numpy_iterator()\n",
    "# noisy = []\n",
    "# gt = []\n",
    "\n",
    "# # Setup Subplot\n",
    "# nrows, ncols = 2, 2\n",
    "# fig, ax = plt.subplots(nrows=nrows, ncols=ncols, sharex=True, figsize=(16, 9))\n",
    "\n",
    "\n",
    "# # iterate over dataset\n",
    "# for i, sample in enumerate(wavs):\n",
    "    \n",
    "#     # get the column and row by modulo and remainder\n",
    "#     j = i % ncols\n",
    "#     k = int(i / ncols)\n",
    "    \n",
    "#     # extract noisy and produced speech file from tensors\n",
    "#     wave = sample[0]\n",
    "#     ground_truth = sample[1]\n",
    "        \n",
    "#     # plot files\n",
    "#     librosa.display.waveshow(np.squeeze(wave), x_axis='time', sr=config['sr'], ax=ax[k][j], label='test_file')\n",
    "#     librosa.display.waveshow(np.squeeze(ground_truth), alpha=0.3, x_axis='time', sr=config['sr'], ax=ax[k][j], label='ground_truth')\n",
    "#     ax[k][j].legend()\n",
    "#     ax[k][j].axis('on')\n",
    "#     ax[k][j].set_title('10s speech')  \n",
    "\n",
    "#     # save speech to arrays\n",
    "#     noisy.append(np.squeeze(wave))\n",
    "#     gt.append(np.squeeze(ground_truth))\n",
    "    \n",
    "#     if i+1 == ncols*nrows:\n",
    "#         break\n",
    "    \n",
    "# # adjust whitespace in between subplots        \n",
    "# plt.subplots_adjust(hspace=0.25, wspace=0.15)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # listen to the audio samples\n",
    "# for i in range(len(gt)):\n",
    "#     print(f'----------- {i+1}. speechsnippet ---------------')\n",
    "#     print('')\n",
    "#     print(f'Voicefixer file')\n",
    "#     pd.display(pd.Audio(noisy[i].T, rate=config['sr']))\n",
    "#     print(f'corresponding produced file')\n",
    "#     pd.display(pd.Audio(gt[i].T, rate=config['sr']))\n",
    "#     print('')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some values for the model\n",
    "input_shape = (3*44100, 1)\n",
    "output_channels = 1\n",
    "filter_size = config['filter_size']\n",
    "kernel_size = config['kernel_size']\n",
    "\n",
    "# build model with 12 layers\n",
    "def build_model(input_shape):\n",
    "\n",
    "    # define model\n",
    "    model = keras.Sequential(name='PostNet_Conv1D')\n",
    "    model.add(keras.Input(shape=input_shape))\n",
    "\n",
    "    # add layer \n",
    "    model.add(keras.layers.Conv1D(filters=filter_size, kernel_size=kernel_size, padding='same'))\n",
    "    model.add(keras.layers.Activation('tanh'))\n",
    "\n",
    "    # Add the remaining Conv1D layers\n",
    "    for _ in range(11):\n",
    "        model.add(keras.layers.Conv1D(filters=filter_size, kernel_size=kernel_size, padding='same'))\n",
    "        model.add(keras.layers.Activation('tanh'))\n",
    "\n",
    "    # Add the final Conv1D layer\n",
    "    model.add(keras.layers.Conv1D(filters=output_channels, kernel_size=1, padding='same'))\n",
    "    model.add(keras.layers.Activation('tanh'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-30 12:56:25.052878: I tensorflow/core/profiler/lib/profiler_session.cc:99] Profiler session initializing.\n",
      "2023-06-30 12:56:25.052899: I tensorflow/core/profiler/lib/profiler_session.cc:114] Profiler session started.\n",
      "2023-06-30 12:56:25.053709: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session tear down.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"PostNet_Conv1D\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_65 (Conv1D)          (None, 132300, 4)         12        \n",
      "                                                                 \n",
      " activation_65 (Activation)  (None, 132300, 4)         0         \n",
      "                                                                 \n",
      " conv1d_66 (Conv1D)          (None, 132300, 4)         36        \n",
      "                                                                 \n",
      " activation_66 (Activation)  (None, 132300, 4)         0         \n",
      "                                                                 \n",
      " conv1d_67 (Conv1D)          (None, 132300, 4)         36        \n",
      "                                                                 \n",
      " activation_67 (Activation)  (None, 132300, 4)         0         \n",
      "                                                                 \n",
      " conv1d_68 (Conv1D)          (None, 132300, 4)         36        \n",
      "                                                                 \n",
      " activation_68 (Activation)  (None, 132300, 4)         0         \n",
      "                                                                 \n",
      " conv1d_69 (Conv1D)          (None, 132300, 4)         36        \n",
      "                                                                 \n",
      " activation_69 (Activation)  (None, 132300, 4)         0         \n",
      "                                                                 \n",
      " conv1d_70 (Conv1D)          (None, 132300, 4)         36        \n",
      "                                                                 \n",
      " activation_70 (Activation)  (None, 132300, 4)         0         \n",
      "                                                                 \n",
      " conv1d_71 (Conv1D)          (None, 132300, 4)         36        \n",
      "                                                                 \n",
      " activation_71 (Activation)  (None, 132300, 4)         0         \n",
      "                                                                 \n",
      " conv1d_72 (Conv1D)          (None, 132300, 4)         36        \n",
      "                                                                 \n",
      " activation_72 (Activation)  (None, 132300, 4)         0         \n",
      "                                                                 \n",
      " conv1d_73 (Conv1D)          (None, 132300, 4)         36        \n",
      "                                                                 \n",
      " activation_73 (Activation)  (None, 132300, 4)         0         \n",
      "                                                                 \n",
      " conv1d_74 (Conv1D)          (None, 132300, 4)         36        \n",
      "                                                                 \n",
      " activation_74 (Activation)  (None, 132300, 4)         0         \n",
      "                                                                 \n",
      " conv1d_75 (Conv1D)          (None, 132300, 4)         36        \n",
      "                                                                 \n",
      " activation_75 (Activation)  (None, 132300, 4)         0         \n",
      "                                                                 \n",
      " conv1d_76 (Conv1D)          (None, 132300, 4)         36        \n",
      "                                                                 \n",
      " activation_76 (Activation)  (None, 132300, 4)         0         \n",
      "                                                                 \n",
      " conv1d_77 (Conv1D)          (None, 132300, 1)         5         \n",
      "                                                                 \n",
      " activation_77 (Activation)  (None, 132300, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 413\n",
      "Trainable params: 413\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------\n",
    "# define callbacks\n",
    "log_dir = \"./logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# make directory if not exist\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "\n",
    "save_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath='./model_checkpoint',\n",
    "    save_best_only=False,\n",
    "    monitor='val_loss',\n",
    "    verbose=0)\n",
    "\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(\n",
    "    log_dir= log_dir,\n",
    "    histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    write_images=True,\n",
    "    write_steps_per_second=True,\n",
    "    update_freq='epoch',\n",
    "    profile_batch=2,\n",
    "    embeddings_freq=1)\n",
    "\n",
    "# early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "#     monitor='val_loss',\n",
    "#     patience=5,\n",
    "#     verbose=1)\n",
    "\n",
    "\n",
    "#--------------------------------------------\n",
    "# helper func\n",
    "def float2pcm(sig, dtype='int16'):\n",
    "    \"\"\"Convert floating point signal with a range from -1 to 1 to PCM.\n",
    "    Any signal values outside the interval [-1.0, 1.0) are clipped.\n",
    "    No dithering is used.\n",
    "    Note that there are different possibilities for scaling floating\n",
    "    point numbers to PCM numbers, this function implements just one of\n",
    "    them.  For an overview of alternatives see\n",
    "    http://blog.bjornroche.com/2009/12/int-float-int-its-jungle-out-there.html\n",
    "    Parameters\n",
    "    ----------\n",
    "    sig : array_like\n",
    "        Input array, must have floating point type.\n",
    "    dtype : data type, optional\n",
    "        Desired (integer) data type.\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Integer data, scaled and clipped to the range of the given\n",
    "        *dtype*.\n",
    "    See Also\n",
    "    --------\n",
    "    pcm2float, dtype\n",
    "    \"\"\"\n",
    "    sig = np.asarray(sig)\n",
    "    if sig.dtype.kind != 'f':\n",
    "        raise TypeError(\"'sig' must be a float array\")\n",
    "    dtype = np.dtype(dtype)\n",
    "    if dtype.kind not in 'iu':\n",
    "        raise TypeError(\"'dtype' must be an integer type\")\n",
    "\n",
    "    i = np.iinfo(dtype)\n",
    "    abs_max = 2 ** (i.bits - 1)\n",
    "    offset = i.min + abs_max\n",
    "    return (sig * abs_max + offset).clip(i.min, i.max).astype(dtype)\n",
    "\n",
    "\n",
    "#--------------------------------------------\n",
    "# set speechfile for prediction\n",
    "dataset = test_dataset.unbatch().as_numpy_iterator()\n",
    "speech_for_predicition = []\n",
    "for i, sample in enumerate(dataset):\n",
    "    speech_for_predicition.append(sample[0])\n",
    "    break\n",
    "# # normalize speechfile\n",
    "speech_for_predicition = speech_for_predicition[0]\n",
    "speech_for_predicition = librosa.util.normalize(speech_for_predicition)\n",
    "\n",
    "# save speech file used for prediction\n",
    "# save plot to disk\n",
    "plt.figure(figsize=(8, 4))\n",
    "x = np.arange(0, len(speech_for_predicition)/44100, 1/44100)\n",
    "plt.plot(x, speech_for_predicition)\n",
    "plt.title('Audiofile')\n",
    "plt.xlabel('Time in s')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.savefig(log_dir + '/_audiofile_for_prediction.png')\n",
    "plt.close()\n",
    "\n",
    "# save audiofile to disk\n",
    "write(log_dir + '/_audiofile_for_prediction' + '.wav', int(44100), float2pcm(speech_for_predicition))\n",
    "\n",
    "\n",
    "#--------------------------------------------\n",
    "# define custom callback\n",
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    \n",
    "    # define functions to happen during training after each epoch\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        # # print(logs) to check metrics\n",
    "        # print('---------------------')\n",
    "        # print(f'Epoch {epoch+1} Metrics:')\n",
    "        # print(logs)\n",
    "        # print('---------------------')\n",
    "        # print('')\n",
    "        \n",
    "\n",
    "        # save predicted audio file after each epoch to disk\n",
    "        # get audio file from model prediciton\n",
    "        audio = self.model.predict(speech_for_predicition)\n",
    "\n",
    "        # change shape to (len(audio), 1)\n",
    "        audio = tf.squeeze(audio, axis=-1)\n",
    "        audio = tf.squeeze(audio, axis=-1).numpy()\n",
    "\n",
    "        print(audio.shape)\n",
    "\n",
    "        # normalize audio with numpy\n",
    "        # m = np.max(np.abs(audio))\n",
    "        # audio = (audio / m).astype(np.float32)\n",
    "        # audio = librosa.util.normalize(audio).astype(np.float32)\n",
    "\n",
    "        # save plot to disk\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        x = np.arange(0, len(audio)/44100, 1/44100)\n",
    "        plt.plot(x, audio)\n",
    "        plt.title('Audiofile')\n",
    "        plt.xlabel('Time in s')\n",
    "        plt.ylabel('Amplitude')\n",
    "        plt.savefig(log_dir + '/_audiofile_epoch' + str(epoch+1) + '.png')\n",
    "        plt.close()\n",
    "\n",
    "        # write audio file to disk (16-bit PCM WAV)\n",
    "        write(log_dir + '/_audiofile_epoch' + str(epoch+1) + '.wav', 44100, float2pcm(audio))\n",
    "\n",
    "\n",
    "#--------------------------------------------\n",
    "# save audio files from logs to tf.summary.audio and event files for tensorboard\n",
    "def save_audio_to_summaries():\n",
    "    \n",
    "        # get audio files from logs\n",
    "        fps = glob.glob(log_dir + '/*.wav')\n",
    "        path_audiosummary = log_dir + '/audiosummary/'\n",
    "\n",
    "        # for each audio file\n",
    "        for idx, fp in enumerate(fps):\n",
    "            \n",
    "            # write audiosummary of one audio file to disk\n",
    "             writer = tf.summary.create_file_writer(path_audiosummary)\n",
    "             with writer.as_default():\n",
    "                \n",
    "                # load audio file as tensor\n",
    "                file = tf.io.read_file(fp)\n",
    "                audio = tf.audio.decode_wav(file, desired_channels=1)\n",
    "\n",
    "                # write audio file to tf.summary.audio\n",
    "                name = fp.split('/')[-1]\n",
    "                tf.summary.audio(name , tf.expand_dims(audio[0], 0), int(44100), step=idx)\n",
    "                writer.flush()\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------\n",
    "# get model\n",
    "model = build_model(input_shape = input_shape)\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer = keras.optimizers.Adam(learning_rate=0.06),\n",
    "              loss = tf.keras.losses.MeanSquaredError(),\n",
    "              metrics = tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown - 2s 2s/step - loss: 0.0261 - mean_squared_error: 0.0261"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-30 12:56:34.542119: I tensorflow/core/profiler/lib/profiler_session.cc:99] Profiler session initializing.\n",
      "2023-06-30 12:56:34.542138: I tensorflow/core/profiler/lib/profiler_session.cc:114] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2/Unknown - 2s 397ms/step - loss: 0.1750 - mean_squared_error: 0.1750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-30 12:56:34.921473: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2023-06-30 12:56:34.923467: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session tear down.\n",
      "2023-06-30 12:56:34.933213: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./logs/20230630-125625/plugins/profile/2023_06_30_12_56_34\n",
      "\n",
      "2023-06-30 12:56:34.933927: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to ./logs/20230630-125625/plugins/profile/2023_06_30_12_56_34/Mariuss-MacBook-Pro.local.trace.json.gz\n",
      "2023-06-30 12:56:34.936554: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./logs/20230630-125625/plugins/profile/2023_06_30_12_56_34\n",
      "\n",
      "2023-06-30 12:56:34.936662: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to ./logs/20230630-125625/plugins/profile/2023_06_30_12_56_34/Mariuss-MacBook-Pro.local.memory_profile.json.gz\n",
      "2023-06-30 12:56:34.937169: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./logs/20230630-125625/plugins/profile/2023_06_30_12_56_34\n",
      "Dumped tool data for xplane.pb to ./logs/20230630-125625/plugins/profile/2023_06_30_12_56_34/Mariuss-MacBook-Pro.local.xplane.pb\n",
      "Dumped tool data for overview_page.pb to ./logs/20230630-125625/plugins/profile/2023_06_30_12_56_34/Mariuss-MacBook-Pro.local.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to ./logs/20230630-125625/plugins/profile/2023_06_30_12_56_34/Mariuss-MacBook-Pro.local.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to ./logs/20230630-125625/plugins/profile/2023_06_30_12_56_34/Mariuss-MacBook-Pro.local.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to ./logs/20230630-125625/plugins/profile/2023_06_30_12_56_34/Mariuss-MacBook-Pro.local.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     30/Unknown - 10s 284ms/step - loss: 0.0396 - mean_squared_error: 0.0396"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 13). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 132300, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 132300, 1), dtype=tf.float32, name='input_6'), name='input_6', description=\"created by layer 'input_6'\"), but it was called on an input with incompatible shape (None, 1, 1).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 132300, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 132300, 1), dtype=tf.float32, name='input_6'), name='input_6', description=\"created by layer 'input_6'\"), but it was called on an input with incompatible shape (None, 1, 1).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4135/4135 [==============================] - 2s 501us/step\n",
      "(132300,)\n",
      "30/30 [==============================] - 16s 484ms/step - loss: 0.0396 - mean_squared_error: 0.0396 - val_loss: 0.0157 - val_mean_squared_error: 0.0157\n",
      "Epoch 2/2\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.0215 - mean_squared_error: 0.0215"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 13). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4135/4135 [==============================] - 2s 500us/step\n",
      "(132300,)\n",
      "30/30 [==============================] - 15s 474ms/step - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0153 - val_mean_squared_error: 0.0153\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=config['n_epochs'],\n",
    "                    validation_data=test_dataset,\n",
    "                    callbacks=[save_callback, tensorboard_callback, CustomCallback()])\n",
    "\n",
    "# save model\n",
    "model.save('./model.h5')\n",
    "\n",
    "# save history\n",
    "with open('./history.json', 'w+') as fp:\n",
    "    json.dump(history.history, fp, sort_keys=True, indent=4)\n",
    "\n",
    "\n",
    "# call def to save audio to summaries\n",
    "save_audio_to_summaries()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mse\n",
    "train_mse = history.history['mean_squared_error']\n",
    "eval_mse = history.history['val_mean_squared_error']\n",
    "\n",
    "fig2 = plt.figure()\n",
    "plt.plot(range(config['n_epochs']), train_mse, label='train')\n",
    "plt.plot(range(config['n_epochs']), eval_mse, label='test')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('mse')\n",
    "plt.title('Training with ' \n",
    "                           + str(config['n_epochs'])\n",
    "                           + ' epochs \\n batch-size: '\n",
    "                           + str(config['batch_size']))\n",
    "                     \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstructed_model = keras.models.load_model('./model.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Masterarbeit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
