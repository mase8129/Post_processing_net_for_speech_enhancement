{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and save dataset as tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "\n",
    " # TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "print('Tensorflow Version:', tf.__version__)\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import os\n",
    "import os.path\n",
    "import glob\n",
    "import librosa\n",
    "import librosa.display\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import IPython.display as pd\n",
    "import pprint\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "# Check if the GPU is available (otherwise computing will take a looooonnnnggggg time)\n",
    "print(\"GPU\", \"available (YESS!!!!)\" if tf.config.list_physical_devices(\"GPU\") else \"not available :(\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load global settings in config-dictionary\n",
    "with open('./MA_CONFIG.json', 'r') as fp:\n",
    "  config = json.load(fp)\n",
    "\n",
    "# define some extra values\n",
    "config['input_shape'] = (441000, 1)\n",
    "\n",
    "# print config\n",
    "print(json.dumps(config, indent=4))\n",
    "\n",
    "# save config to disk\n",
    "with open('./MA_CONFIG.json', 'w+') as fp:\n",
    "    json.dump(config, fp, sort_keys=True, indent=4)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_process_data(file_path):\n",
    "\n",
    "    # paths for ground truths prod files\n",
    "    fps_prod = glob.glob('/Users/marius/Documents/Uni/TU_Berlin_Master/Masterarbeit/Data/allFiles/producedSpeech/**.wav')\n",
    "\n",
    "    # path string is saved as byte array in tf.data.dataset -> convert back to str\n",
    "    if type(file_path) is not str:\n",
    "        file_path = file_path.numpy()\n",
    "        file_path = file_path.decode('utf-8')\n",
    "    \n",
    "    # load audio data \n",
    "    y, _ = librosa.core.load(file_path, sr=config['sr'], mono=True, offset=0.0, duration=None, \n",
    "                             dtype=np.float32, res_type='kaiser_fast')\n",
    "\n",
    "\n",
    "    # get string with speaker and scriptname\n",
    "    label = file_path.split('/')[-1]\n",
    "    label = label[:10]\n",
    "    for filename in fps_prod:\n",
    "        if label in filename:\n",
    "            fp = filename\n",
    "            break\n",
    "\n",
    "    # load corresponding produced audio file\n",
    "    y_truth, _ = librosa.core.load(fp, sr=config['sr'], mono=True, offset=0.0, duration=None, \n",
    "                             dtype=np.float32, res_type='kaiser_fast')\n",
    "    \n",
    "\n",
    "    # cut audio into 10s frames\n",
    "    seg = 10*44100\n",
    "    y_10s = librosa.util.frame(y, frame_length=seg, hop_length=seg).T\n",
    "    y_truth_10s = librosa.util.frame(y_truth, frame_length=seg, hop_length=seg).T\n",
    "\n",
    "\n",
    "    # zero pad last segment to seg if not zero padded already\n",
    "    #for i in range(len(y_10s)):\n",
    "    #    if not len(y_10s[i]) == seg:\n",
    "    #        y_10s[i] = librosa.util.fix_length(y_10s[i], size=seg)\n",
    "        \n",
    "    \n",
    "\n",
    "    return y_10s, y_truth_10s\n",
    "\n",
    "\n",
    "def preprocessing_wrapper(file_path):\n",
    "\n",
    "    # execute the preprocessing function\n",
    "    y, y_truth = tf.py_function(load_and_process_data, [file_path], [tf.float32, tf.float32])\n",
    "    \n",
    "    # Input shape for wave inputs (longest file, 1)\n",
    "    #y.set_shape([config['input_shape'][1], 1])\n",
    "    #y_truth.set_shape([config['input_shape'][1], 1])\n",
    "\n",
    "    return y, y_truth\n",
    "\n",
    "def transpose(tensor, tensor_spec):\n",
    "    out = tf.transpose(tensor, perm=[1,0])\n",
    "    return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create Dataset with preprocessing func from wav-files and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autotune computation\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "\n",
    "# folder with the training data\n",
    "train_files = '/Users/marius/Documents/Uni/TU_Berlin_Master/Masterarbeit/Dataset/train/*.wav'\n",
    "# folder with the test data\n",
    "test_files = '/Users/marius/Documents/Uni/TU_Berlin_Master/Masterarbeit/Dataset/test/*.wav'\n",
    "\n",
    "\n",
    "# define a dataset of file paths - TRAIN\n",
    "train_dataset = tf.data.Dataset.list_files(train_files, shuffle=False)\n",
    "# define a dataset of file paths - TEST\n",
    "test_dataset = tf.data.Dataset.list_files(test_files, shuffle=False)\n",
    "\n",
    "\n",
    "# little dataset for testing\n",
    "train_dataset = train_dataset.take(2)\n",
    "# little dataset for testing\n",
    "test_dataset = test_dataset.take(2)\n",
    "\n",
    "\n",
    "# run the preprocessing via map\n",
    "train_dataset = train_dataset.map(preprocessing_wrapper, num_parallel_calls=AUTOTUNE)\n",
    "# run the preprocessing via map\n",
    "test_dataset = test_dataset.map(preprocessing_wrapper, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "\n",
    "\n",
    "print('---------------------------------------')\n",
    "print('-----------TRAIN DATASET---------------')\n",
    "i = len([i for i in train_dataset])\n",
    "print(f\" Number of Tensors before unbatching: {i}\")\n",
    "\n",
    "train_dataset = train_dataset.unbatch()\n",
    "i = len([i for i in train_dataset])\n",
    "print(f\" Number of Tensors after unbatching: {i}\")\n",
    "\n",
    "# Dataset shape should be:\n",
    "# tuple = ((batches, 1, 441000), (batches, 1, 441000))\n",
    "\n",
    "\n",
    "#  # add channel dimension to tuple\n",
    "train_dataset = train_dataset.map(lambda x, y: (tf.expand_dims(x, axis=1), tf.expand_dims(y, axis=1)), num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# check shape of dataset\n",
    "for d in train_dataset:\n",
    "    print('Shapes per element')\n",
    "    print(f'1. Tensor shape: {d[0].shape}')\n",
    "    print(f'2. Tensor shape: {d[1].shape}')\n",
    "    break\n",
    "\n",
    "\n",
    "print('')\n",
    "print('--------------------------------------')\n",
    "print('-----------TEST DATASET---------------')\n",
    "i = len([i for i in test_dataset])\n",
    "print(f\" Number of Tensors before unbatching: {i}\")\n",
    "\n",
    "test_dataset = test_dataset.unbatch()\n",
    "i = len([i for i in test_dataset])\n",
    "print(f\" Number of Tensors after unbatching: {i}\")\n",
    "\n",
    "#  # add channel dimension to tuple\n",
    "test_dataset = test_dataset.map(lambda x, y: (tf.expand_dims(x, axis=1), tf.expand_dims(y, axis=1)), num_parallel_calls=AUTOTUNE)\n",
    "# transpose the tensors\n",
    "#test_dataset = test_dataset.map(transpose, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "\n",
    "\n",
    "# check shape of dataset\n",
    "for d in test_dataset:\n",
    "    print('Shapes per element')\n",
    "    print(f'1. Tensor shape: {d[0].shape}')\n",
    "    print(f'2. Tensor shape: {d[1].shape}')\n",
    "    break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# save datasets to tfrecords\n",
    "\n",
    "# print('')\n",
    "# print('--------------------------------------')\n",
    "# print('-----------SAVING DATASETS---------------')\n",
    "\n",
    "\n",
    "print(f\" saving train dataset to tfrecords...\")\n",
    "\n",
    "# count the time and print it\n",
    "start = time.time()\n",
    "tf.data.experimental.save(train_dataset, '/Users/marius/Documents/Uni/TU_Berlin_Master/Masterarbeit/Dataset/train.tfrecord', compression='GZIP')\n",
    "# print time for saving\n",
    "print(f\" saving train dataset took {time.time()-start} seconds\")\n",
    "\n",
    "\n",
    "print(f\" saving test dataset to tfrecords...\")\n",
    "# count the time and print it\n",
    "start = time.time()\n",
    "tf.data.experimental.save(test_dataset, '/Users/marius/Documents/Uni/TU_Berlin_Master/Masterarbeit/Dataset/test.tfrecord', compression='GZIP')\n",
    "# print time for saving\n",
    "print(f\" saving test dataset took {time.time()-start} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for noise, gt in train_dataset.take(1):  # only take first element of dataset\n",
    "#     noisy_speech = noise.numpy()\n",
    "#     gt_speech = gt.numpy()\n",
    "\n",
    "# print(noisy_speech.shape)\n",
    "# print(gt_speech.shape)\n",
    "\n",
    "\n",
    "# for noise, gt in test_dataset.take(4):  # only take first element of dataset\n",
    "#     noisy_speech = noise.numpy()\n",
    "#     gt_speech = gt.numpy()\n",
    "\n",
    "# print(noisy_speech.shape)\n",
    "# print(gt_speech.shape)\n",
    "\n",
    "# # plot noisy speech\n",
    "# plt.figure(figsize=(10, 4))\n",
    "# plt.title('Noisy speech')\n",
    "# librosa.display.waveplot(np.squeeze(noisy_speech), sr=44100)\n",
    "# plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the Data by plot and audio display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at some example data from train dataset\n",
    "wavs = train_dataset.as_numpy_iterator()\n",
    "noisy = []\n",
    "gt = []\n",
    "\n",
    "# Setup Subplot\n",
    "nrows, ncols = 2, 2\n",
    "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, sharex=True, figsize=(16, 9))\n",
    "\n",
    "\n",
    "# iterate over dataset\n",
    "for i, sample in enumerate(wavs):\n",
    "    \n",
    "    # get the column and row by modulo and remainder\n",
    "    j = i % ncols\n",
    "    k = int(i / ncols)\n",
    "    \n",
    "    # extract noisy and produced speech file from tensors\n",
    "    wave = sample[0]\n",
    "    ground_truth = sample[1]\n",
    "        \n",
    "    # plot files\n",
    "    librosa.display.waveshow(np.squeeze(wave), x_axis='time', sr=config['sr'], ax=ax[k][j], label='test_file')\n",
    "    librosa.display.waveshow(np.squeeze(ground_truth), alpha=0.3, x_axis='time', sr=config['sr'], ax=ax[k][j], label='ground_truth')\n",
    "    ax[k][j].legend()\n",
    "    ax[k][j].axis('on')\n",
    "    ax[k][j].set_title('10s speech')  \n",
    "\n",
    "    # save speech to arrays\n",
    "    noisy.append(np.squeeze(wave))\n",
    "    gt.append(np.squeeze(ground_truth))\n",
    "    \n",
    "    if i+1 == ncols*nrows:\n",
    "        break\n",
    "    \n",
    "# adjust whitespace in between subplots        \n",
    "plt.subplots_adjust(hspace=0.25, wspace=0.15)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# listen to the audio samples\n",
    "for i in range(len(gt)):\n",
    "    print(f'----------- {i+1}. speechsnippet ---------------')\n",
    "    print('')\n",
    "    print(f'Voicefixer file')\n",
    "    pd.display(pd.Audio(noisy[i].T, rate=config['sr']))\n",
    "    print(f'corresponding produced file')\n",
    "    pd.display(pd.Audio(gt[i].T, rate=config['sr']))\n",
    "    print('')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Masterarbeit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
